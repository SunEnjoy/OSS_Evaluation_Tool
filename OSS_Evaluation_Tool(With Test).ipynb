{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn import preprocessing\n",
    "\n",
    "# URL exampleï¼š\n",
    "# url = 'https://api.github.com/search/repositories?q=language:python&sort=starts'\n",
    "# # Here l is the language, q is the query search term and type is the search range\n",
    "# url = 'https://api.github.com/search/repositories?l=Python&q=translator&type=Repositories'\n",
    "# # We can also do a direct search for the readme, not for the keywords in the project itself but for the content of the readme.\n",
    "# url = 'https://api.github.com/search/repositories?q=in%3Areadme+web+starts%3A%3E1000&type=Repositories'\n",
    "\n",
    "myToken = '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'\n",
    "client_id='83352693b87485370e8a'\n",
    "client_secret='f6cc77a83ca7303342bc5716427aacfd92c319be'\n",
    "# b16d85bab6f9937a202e1127ca03d8490fa394a3\n",
    "\n",
    "# Change sort way\n",
    "def getGitURL(keyword,sortway):\n",
    "    url = 'https://api.github.com/search/repositories?q={keyword}&s={sortway}&type=Repositories'.format(keyword=keyword,sortway=sortway)\n",
    "\n",
    "    return url\n",
    "\n",
    "# Returns the responses obtained according to 4 different sorts\n",
    "def getKeywordResponses(keyword):\n",
    "    url = getGitURL(keyword,\"stars\")\n",
    "    r_stars = requests.get(url,auth = (client_id, client_secret))\n",
    "    \n",
    "    url = getGitURL(keyword,\"\")\n",
    "    r_bestMatch = requests.get(url,auth = (client_id, client_secret))\n",
    "    \n",
    "    url = getGitURL(keyword,\"forks\")\n",
    "    r_forks = requests.get(url,auth = (client_id, client_secret))\n",
    "    \n",
    "    url = getGitURL(keyword,\"updated\")\n",
    "    r_updated= requests.get(url,auth = (client_id, client_secret))\n",
    "\n",
    "#     print(\"Status Code: \", r_stars.status_code)\n",
    "#     print(\"Status Code: \", r_bestMatch.status_code)\n",
    "#     print(\"Status Code: \", r_forks.status_code)\n",
    "#     print(\"Status Code: \", r_updated.status_code)\n",
    "    \n",
    "    return [r_stars,r_bestMatch,r_forks,r_updated]\n",
    "\n",
    "\n",
    "def getURLgroup(responses):\n",
    "    dict1 = dict()\n",
    "    for res in responses:\n",
    "#         getRepositoryInfomation(res)\n",
    "        res_dict = res.json()\n",
    "        if 'items' in res_dict.keys():\n",
    "            r_dicts = res_dict['items']\n",
    "            # Here the URLs obtained by traversing each search sort are stored in a dictionary,\n",
    "            # the key of which is the URL and the value is the number of occurrences.\n",
    "            for i in r_dicts:\n",
    "                dict1.update([(i['html_url'], dict1[i['html_url']] + 1)]) if i['html_url'] in dict1 else dict1.update([(i['html_url'], 1)])\n",
    "    return dict1\n",
    "        \n",
    "    \n",
    "        \n",
    "# Enter a native URL, return a URL that can call the api\n",
    "# rawURL: https://github.com/pingfangx/TranslatorX\n",
    "# return: https://api.github.com/repos/pingfangx/TranslatorX\n",
    "def githubAPIadaptor(rawURL):\n",
    "    index = rawURL.find(r'github') # Find 'api' index\n",
    "    index2 = rawURL.find(r'github.com/')+len(r'github.com/')\n",
    "    return rawURL[:index] + 'api.' + rawURL[index:index2]+r'repos/'+rawURL[index2:]\n",
    "\n",
    "def timestampToSec(date):\n",
    "    # Transfer Date to Timestamp\n",
    "    t = time.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time.mktime(t)\n",
    "\n",
    "def MaxMinNormalization(x):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x = min_max_scaler.fit_transform(x)\n",
    "#     if x.max(axis=0).all()==x.min(axis=0).all():\n",
    "#         return (x - x.min(axis=0))/(x.max(axis=0)-x.min(axis=0))\n",
    "#     else:\n",
    "#         print(x)\n",
    "    return x\n",
    "\n",
    "def checkDocument(base):\n",
    "    my_dict = {}\n",
    "    # Document Keywords\n",
    "    checkList = [\"READ\",\"CONTRIBUTING\",\"LICENSE\",\"CODE\",\"REQUEST\",\"REMAINING\"]\n",
    "    for c in checkList:\n",
    "        my_dict[c] = 0.0\n",
    "    # Locate the Community page\n",
    "    myURL = base + \"/community\"\n",
    "    response = requests.get(myURL,auth = (client_id,client_secret))\n",
    "    response.encoding = 'utf-8'\n",
    "    mySoup = BeautifulSoup(response.text, 'html.parser')  #HTML File\n",
    "    found = 0\n",
    "    # Find all hyperlinked Tags\n",
    "    for k in mySoup.find_all('a'):\n",
    "        link = k.get('href')\n",
    "        if link is not None and \"README.md\" in str(link):\n",
    "            my_dict[\"READ\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"CONTRIBUTING.md\" in str(link):\n",
    "            my_dict[\"CONTRIBUTING\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"LICENSE\" in str(link):\n",
    "            my_dict[\"LICENSE\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"CODE_OF_CONDUCT.md\" in str(link):\n",
    "            my_dict[\"CODE\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"PULL_REQUEST_TEMPLATE\" in str(link):\n",
    "            my_dict[\"REQUEST\"] = 1\n",
    "            found += 1\n",
    "            \n",
    "    flag_list = ['octicon', 'octicon-check', 'mr-1', 'color-fg-success']\n",
    "    total = 0.0\n",
    "    for k in mySoup.find_all('svg',{\"aria-label\" : \"Added\"}):\n",
    "        if k.get(\"class\") == flag_list:\n",
    "            total = total + 1\n",
    "    \n",
    "    my_dict[\"REMAINING\"] = total - found\n",
    "    return my_dict\n",
    "\n",
    "def checkInput(user_input,keyword=True):\n",
    "    user_input = user_input.strip().lower()\n",
    "    if keyword:\n",
    "        return True if re.match(r'^(?=.*[a-zA-Z])|(?=.*[0-9])', user_input) else False\n",
    "    else:\n",
    "        return True if user_input.startswith( r'https://github.com/') else False\n",
    "\n",
    "def evaluateRepo(repo):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list_keyword = [\"\",\"  \",\"    \",\"!#$#@!#!@\",\"!~%&(^$,\",\"  sss3 \",\"321\",\"  88\",\"None\",\"Null\"]\n",
    "test_list_URL = [\"@#~~!#&^\",\"!@sunLL#$,\",\"https://www.youtube.com/\",\n",
    "                 \"https://www.4399.com/\",\"https://gitee.com/\",\"https://GiTnUb.com/\",\n",
    "                 \"https://GiThUb.com/\",\"https://gitHUB.com/\",\"https://minerva.leeds.ac.uk/\",\n",
    "                 \"https://docs.python.org/3/library/datetime.html\"\n",
    "                ]\n",
    "for k in test_list_keyword:\n",
    "    print(''' Input String: \"{Input}\": {space}\\t Result: '''.format(Input = k, space = \" \"*2),checkInput(k))\n",
    "for k in test_list_URL:\n",
    "    print(''' Input String: \"{Input}\": {space}\\t Result: '''.format(Input = k, space = \" \"*1),checkInput(k,False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_list_keyword)+len(test_list_URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the URLs for further analysis and store them in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getRepositoryInfomation(getKeywordResponses('translator')[0])\n",
    "# keyword_list = []\n",
    "\n",
    "#     time_start=time.time()\n",
    "res = getURLgroup(getKeywordResponses('translator'))\n",
    "#     time_end=time.time()\n",
    "#     keyword_list.append(time_end-time_start)\n",
    "#     print('Get Keyword Query Result:',time_end-time_start,'s')\n",
    "\n",
    "# print(len(res))\n",
    "for k,v in res.items():\n",
    "    print(k,v)\n",
    "#     print(checkInput(k,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through all the URLs in the dict for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arr = np.array([[1.1,2.2,3.3,4.4,5.5,6.6,7.7,8.8,9.9,10.1,11.2,12.3,23.1,3.3,4.4]])\n",
    "# items_metadata_time = []\n",
    "# timedata_conversion = []\n",
    "# checkDocument_result = []\n",
    "# insertion_time = []\n",
    "for k,v in res.items():\n",
    "    apiURL = githubAPIadaptor(k)\n",
    "#     time_start = time.time()\n",
    "    r = requests.get(apiURL,auth = (client_id, client_secret))\n",
    "#     time_end = time.time()\n",
    "#     items_metadata_time.append(time_end-time_start)\n",
    "#     print('Get Item Metadata:',time_end-time_start,'s')\n",
    "    \n",
    "#     print(r.headers['X-RateLimit-Remaining'])\n",
    "    response_dict = r.json()\n",
    "    \n",
    "#     time_start = time.time()\n",
    "    update_sec = timestampToSec(response_dict['updated_at'])\n",
    "    push_sec = timestampToSec(response_dict['pushed_at'])\n",
    "#     time_end = time.time()\n",
    "#     timedata_conversion.append(time_end-time_start)\n",
    "#     print('Time data conversion:',time_end-time_start,'s')\n",
    "    \n",
    "    has_wiki = 1 if response_dict['has_wiki'] is True else 0\n",
    "    # Check if Description and Issue_Template exist\n",
    "    \n",
    "#     time_start = time.time()\n",
    "    checkList = checkDocument(k)\n",
    "#     time_end = time.time()\n",
    "#     checkDocument_result.append(time_end-time_start)\n",
    "#     print('Get Item HTML data:',time_end-time_start,'s')\n",
    "    \n",
    "    if checkList[\"REMAINING\"] == 2:\n",
    "        checkList[\"ISSUE\"] = 1\n",
    "    elif checkList[\"REMAINING\"] == 1 and response_dict[\"description\"] == None:\n",
    "        checkList[\"ISSUE\"] = 1\n",
    "    elif checkList[\"REMAINING\"] == 1 and response_dict[\"description\"] != None:\n",
    "        checkList[\"Description\"] = 1\n",
    "    \n",
    "#     time_start = time.time()\n",
    "    row = np.array([update_sec,\n",
    "                    push_sec,\n",
    "                    response_dict['watchers_count'],\n",
    "                    response_dict['stargazers_count'],\n",
    "                    response_dict['forks'],\n",
    "                    response_dict['open_issues'],\n",
    "                    response_dict['subscribers_count'],\n",
    "                    has_wiki,\n",
    "                    checkList.get(\"READ\",0.0),\n",
    "                    checkList.get(\"Description\",0.0),\n",
    "                    checkList.get(\"CONTRIBUTING\",0.0),\n",
    "                    checkList.get(\"LICENSE\",0.0),\n",
    "                    checkList.get(\"CODE\",0.0),\n",
    "                    checkList.get(\"REQUEST\",0.0),\n",
    "                    checkList.get(\"ISSUE\",0.0)\n",
    "                   ])\n",
    "    row_n = arr.shape[0] ##last row\n",
    "    arr = np.insert(arr,row_n,[row],axis= 0)\n",
    "#     time_end = time.time()\n",
    "#     insertion_time.append(time_end-time_start)\n",
    "#     print('Data Insertion:',time_end-time_start,'s')\n",
    "    \n",
    "arr = np.delete(arr,0,axis = 0)\n",
    "# print(arr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_data = MaxMinNormalization(arr)\n",
    "# print(normed_data)\n",
    "update=1.5\n",
    "push = 1.5\n",
    "watcher = 0.6\n",
    "star = 0.8\n",
    "fork = 0.6\n",
    "open_issue = 0.4 if arr[6].any() > 0.2 else 0.1\n",
    "subscriber = 0.6\n",
    "has_wiki = 1.0\n",
    "read = 1.8\n",
    "description = 0.4 \n",
    "contributing = 0.2\n",
    "license = 0.2\n",
    "code_conduct = 0.2 \n",
    "pull_req = 0.2\n",
    "issue = 0.2\n",
    "normed_data*=np.array([update,push,watcher,star,fork,\n",
    "                       open_issue,subscriber,has_wiki,\n",
    "                       read,description,contributing,\n",
    "                       license,code_conduct,pull_req,issue])\n",
    "normed_data = np.around(normed_data,5)\n",
    "# print(normed_data)\n",
    "sum_data = np.sum(normed_data,axis=1)\n",
    "print(sum_data)\n",
    "# Find the index of the one with the highest score.\n",
    "higestScore = np.where(sum_data==np.max(sum_data))[0][0]\n",
    "print(higestScore)\n",
    "print(list(res.items())[higestScore])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis = []\n",
    "for i in range(100):\n",
    "    time_start = time.time()\n",
    "    normed_data = MaxMinNormalization(arr)\n",
    "    normed_data*=np.array([update,push,watcher,star,fork,\n",
    "                           open_issue,subscriber,has_wiki,\n",
    "                           read,description,contributing,\n",
    "                           license,code_conduct,pull_req,issue])\n",
    "    normed_data = np.around(normed_data,5)\n",
    "    sum_data = np.sum(normed_data,axis=1)\n",
    "\n",
    "    higestScore = np.where(sum_data==np.max(sum_data))[0][0]\n",
    "    time_end = time.time()\n",
    "    data_analysis.append(time_end-time_start)\n",
    "    print(time_end-time_start)\n",
    "\n",
    "print(\"Average time spent on data analysis:\",sum(data_analysis)/len(data_analysis),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_keyword_query = sum(keyword_list)/len(keyword_list)\n",
    "av_metadata = sum(items_metadata_time)/len(items_metadata_time)\n",
    "av_time_conversion = sum(timedata_conversion)/len(timedata_conversion)\n",
    "av_html_data = sum(checkDocument_result)/len(checkDocument_result)\n",
    "av_insertion_data = sum(insertion_time)/len(insertion_time)\n",
    "av_normalization = sum(normalization_time)/len(normalization_time)\n",
    "av_data_analysis = sum(data_analysis)/len(data_analysis)\n",
    "\n",
    "\n",
    "\n",
    "totoal = av_keyword_query + av_metadata + av_time_conversion + av_html_data + av_insertion_data + av_normalization + av_data_analysis\n",
    "\n",
    "print(\"Average time spent on getting keyword query result:\",av_keyword_query,\"s\",(av_keyword_query/totoal)*100,\"%\")\n",
    "\n",
    "print(\"Average time spent on getting item metadata:\",av_metadata,\"s\",(av_metadata/totoal)*100,\"%\")\n",
    "\n",
    "print(\"Average time spent on time data conversion:\",av_time_conversion,\"s\",(av_time_conversion/totoal)*100,\"%\")\n",
    "\n",
    "print(\"Average time spent on getting item HTML data:\",av_html_data,\"s\",(av_html_data/totoal)*100,\"%\")\n",
    "\n",
    "print(\"Average time spent on normaliztion:\",av_normalization,\"s\",(av_normalization/totoal)*100,\"%\")\n",
    "\n",
    "print(\"Average time spent on data insertion:\",av_insertion_data,\"s\",(av_insertion_data/totoal)*100,\"%\")\n",
    "\n",
    "print(\"Average time spent on data analysis:\",av_data_analysis,\"s\",(av_data_analysis/totoal)*100,\"%\")\n",
    "\n",
    "print(\"Total:\",totoal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = r'https://github.com/pingfangx/TranslatorX'\n",
    "index = my_string.find(r'github') # Found 'api'\n",
    "index2 = my_string.find(r'github.com/')+len(r'github.com/')\n",
    "final_string = my_string[:index] + 'api.' + my_string[index:index2]+r'repos/'+my_string[index2:]\n",
    "print(final_string)\n",
    "print(final_string=='https://api.github.com/repos/pingfangx/TranslatorX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata for individual items based on URL\n",
    "url = 'https://api.github.com/repos/apache/spark'\n",
    "# URLï¼šhttps://github.com/pingfangx/TranslatorX\n",
    "url = 'https://api.github.com/repos/pingfangx/TranslatorX'\n",
    "# For example, \n",
    "# for https://github.com/torvalds/linux, \n",
    "# you'd fetch https://api.github.com/repos/torvalds/linux and parse the information from the JSON.\n",
    "\n",
    "r = requests.get(url)\n",
    "print(\"Status Code: \", r.status_code)\n",
    "print(type(r))\n",
    "\n",
    "response_dict = r.json()\n",
    "# print(response_dict.keys())\n",
    "\n",
    "for k in response_dict.keys():\n",
    "    print(k,response_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id='83352693b87485370e8a'\n",
    "client_secret='f7b3442ab29834db012eddfa418654ea1b854522'\n",
    "\n",
    "hea = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.118 Safari/537.36'}\n",
    " \n",
    "html = requests.get('https://github.com/junhaideng/github-dl-tool',auth = (client_id,client_secret))\n",
    " \n",
    "html.encoding = 'utf-8' \n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html.text, 'html.parser') \n",
    "for k in soup.find_all('a'):\n",
    "    link = k.get('href')\n",
    "    if link is not None and \"README.\" in str(link):\n",
    "        print(link)\n",
    "\n",
    "# title = re.findall(\"a href=.*>README.md</a>\",html.text)\n",
    "# for each in title: \n",
    "#     print(each)\n",
    "# print(html.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pingfangx/TranslatorX  Original URL\n",
    "# https://github.com/pingfangx/TranslatorX/community\n",
    "# Contributingï¼šhttps://github.com/pingfangx/TranslatorX/blob/master/CONTRIBUTING.md\n",
    "# Licenseï¼šhttps://github.com/WirePact/k8s-basic-auth-translator/blob/main/LICENSE\n",
    "# Code of conduct: https://github.com/microsoft/.github/blob/main/CODE_OF_CONDUCT.md\n",
    "html = requests.get('https://github.com/microsoft/.github/blob/main/CODE_OF_CONDUCT.md',auth = (client_id,client_secret))\n",
    "print(html.status_code)\n",
    "html = requests.get('https://github.com/WirePact/k8s-basic-auth-translator/blob/main/CODE_OF_CONDUCT.md',auth = (client_id,client_secret))\n",
    "print(html.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://github.com/apache/spark/community',auth = (client_id,client_secret))\n",
    " \n",
    "html.encoding = 'utf-8' \n",
    "\n",
    "soup = BeautifulSoup(html.text, 'html.parser') \n",
    "for k in soup.find_all('a'):\n",
    "    link = k.get('href')\n",
    "    if link is not None and \"README.md\" in str(link):\n",
    "        print(link)\n",
    "    if link is not None and \"CONTRIBUTING.md\" in str(link):\n",
    "        print(link)\n",
    "    if link is not None and \"LICENSE\" in str(link):\n",
    "        print(link)\n",
    "    if link is not None and \"CODE_OF_CONDUCT.md\" in str(link):\n",
    "        print(link)\n",
    "    if link is not None and \"PULL_REQUEST_TEMPLATE\" in str(link):\n",
    "        print(link)\n",
    "    if link is not None and \"ISSUE_TEMPLATES\" in str(link):\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal URL: https://github.com/SunEnjoy/JustARep\")\n",
    "print(\"GitHub API URL:\",githubAPIadaptor(\"https://github.com/SunEnjoy/JustARep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = getGitURL(\"spark\",\"stars\")\n",
    "r_stars = requests.get(url,auth = (client_id, client_secret))\n",
    "res_dict = r_stars.json()\n",
    "print(res_dict.keys())\n",
    "print(\"Total Count:\",res_dict[\"total_count\"])\n",
    "print(\"Incomplete_results:\",res_dict[\"incomplete_results\"])\n",
    "print(\"Number of Returned Items:\",len(res_dict[\"items\"]))\n",
    "\n",
    "print(\"Number of keys in Each Item:\",len(res_dict[\"items\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "print(len(res_dict[\"items\"][0][\"owner\"]))\n",
    "print(res_dict[\"items\"][0][\"owner\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = getGitURL(\"trans\",\"stars\")\n",
    "r_stars = requests.get(url,auth = (client_id, client_secret))\n",
    "res_dict = r_stars.json()\n",
    "print(res_dict.keys())\n",
    "print(\"Total Count:\",res_dict[\"total_count\"])\n",
    "print(\"Incomplete_results:\",res_dict[\"incomplete_results\"])\n",
    "print(\"Number of Returned Items:\",len(res_dict[\"items\"]))\n",
    "print(\"Number of keys in Each Item:\",len(res_dict[\"items\"][0]))\n",
    "print(\"Number of keys in Each Item:\",res_dict[\"items\"][0]['has_wiki'])\n",
    "\n",
    "has_wiki = 1 if res_dict[\"items\"][0]['has_wiki'] is True else 0\n",
    "\n",
    "print(\"Number of keys in Each Item:\",has_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of keys in Each Item:\",res_dict['has_wiki'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "response = requests.get(\"https://github.com/jenkinsci/docker/community\",auth = (client_id,client_secret))\n",
    "\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "mySoup = BeautifulSoup(response.text, 'html.parser')  #HTML File\n",
    "    # Find all hyperlinked Tags\n",
    "flag_list = ['octicon', 'octicon-check', 'mr-1', 'color-fg-success']\n",
    "\n",
    "for k in mySoup.find_all('svg',{\"aria-label\" : \"Added\"}):\n",
    "    if k.get(\"class\") == flag_list:\n",
    "        print(\"yyyyyy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Words Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_word import RandomWords\n",
    "# Generate Random Words\n",
    "r = RandomWords()\n",
    "# Count the number of times the site appears in repetitions 1, 2, 3 and 4\n",
    "value_count = {}\n",
    "\n",
    "for i in range(50):\n",
    "    res = getURLgroup(getKeywordResponses(r.get_random_word()))\n",
    "    for v in res.values():\n",
    "        value_count[v] = value_count.get(v, 0) + 1\n",
    "\n",
    "for k,v in value_count.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "cddedc34-befe-42c0-8899-6b64d1a8edd3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
