{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn import preprocessing\n",
    "\n",
    "myToken = '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'\n",
    "client_id='83352693b87485370e8a'\n",
    "client_secret='f6cc77a83ca7303342bc5716427aacfd92c319be'\n",
    "oneMonth = 2592000\n",
    "update=1.5\n",
    "push = 1.5\n",
    "watcher = 0.6\n",
    "star = 0.8\n",
    "fork = 0.6\n",
    "open_issue = 0.4\n",
    "subscriber = 0.6\n",
    "has_wiki = 1.0\n",
    "read = 1.8\n",
    "description = 0.4 \n",
    "contributing = 0.2\n",
    "license = 0.2\n",
    "code_conduct = 0.2 \n",
    "pull_req = 0.2\n",
    "issue = 0.2\n",
    "default_weight = np.array([update,push,watcher,star,fork,\n",
    "                       open_issue,subscriber,has_wiki,\n",
    "                       read,description,contributing,\n",
    "                       license,code_conduct,pull_req,issue])\n",
    "\n",
    "def getGitURL(keyword,sortway):\n",
    "    url = 'https://api.github.com/search/repositories?q={keyword}&s={sortway}&type=Repositories'.format(keyword=keyword,sortway=sortway)\n",
    "\n",
    "    return url\n",
    "\n",
    "# Returns the responses obtained according to 4 different sorts\n",
    "def getKeywordResponses(keyword):\n",
    "    url = getGitURL(keyword,\"stars\")\n",
    "    r_stars = requests.get(url,auth = (client_id, client_secret))\n",
    "    \n",
    "    url = getGitURL(keyword,\"\")\n",
    "    r_bestMatch = requests.get(url,auth = (client_id, client_secret))\n",
    "    \n",
    "    url = getGitURL(keyword,\"forks\")\n",
    "    r_forks = requests.get(url,auth = (client_id, client_secret))\n",
    "    \n",
    "    url = getGitURL(keyword,\"updated\")\n",
    "    r_updated= requests.get(url,auth = (client_id, client_secret))\n",
    "\n",
    "    \n",
    "    return [r_stars,r_bestMatch,r_forks,r_updated]\n",
    "\n",
    "\n",
    "def getURLgroup(responses):\n",
    "    dict1 = dict()\n",
    "    for res in responses:\n",
    "        res_dict = res.json()\n",
    "        if 'items' in res_dict.keys():\n",
    "            r_dicts = res_dict['items']\n",
    "            # Here the URLs obtained by traversing each search sort are stored in a dictionary,\n",
    "            # the key of which is the URL and the value is the number of occurrences.\n",
    "            for i in r_dicts:\n",
    "                dict1.update([(i['html_url'], dict1[i['html_url']] + 1)]) if i['html_url'] in dict1 else dict1.update([(i['html_url'], 1)])\n",
    "    return dict1\n",
    "        \n",
    "    \n",
    "def getRepositoryInfomation(r):\n",
    "    response_dict = r.json()\n",
    "    print(response_dict.keys())\n",
    "    print(\"Toal:\", response_dict['total_count'])\n",
    "    repo_dicts = response_dict['items']\n",
    "    print(\"Repositories number:\", len(repo_dicts))\n",
    "    for repo_dict in repo_dicts:\n",
    "        print('Name:', repo_dict['name'])\n",
    "        print('Owner:', repo_dict['owner']['login'])\n",
    "        print('Stars:', repo_dict['stargazers_count'])\n",
    "        print('URL:', repo_dict['html_url'])\n",
    "        print('Created_At:', repo_dict['created_at'])\n",
    "        print('Updated_At:', repo_dict['updated_at'])\n",
    "        print('Description:', repo_dict['description'])\n",
    "\n",
    "\n",
    "def githubAPIadaptor(rawURL):\n",
    "    index = rawURL.find(r'github') # Find 'api' index\n",
    "    index2 = rawURL.find(r'github.com/')+len(r'github.com/')\n",
    "    return rawURL[:index] + 'api.' + rawURL[index:index2]+r'repos/'+rawURL[index2:]\n",
    "\n",
    "def timestampToSec(date):\n",
    "    # Transfer Date to Timestamp\n",
    "    t = time.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    return time.mktime(t)\n",
    "\n",
    "def MaxMinNormalization(x):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x = min_max_scaler.fit_transform(x)\n",
    "    return x\n",
    "\n",
    "def checkDocument(base):\n",
    "    my_dict = {}\n",
    "    # Document Keywords\n",
    "    checkList = [\"READ\",\"CONTRIBUTING\",\"LICENSE\",\"CODE\",\"REQUEST\",\"REMAINING\"]\n",
    "    for c in checkList:\n",
    "        my_dict[c] = 0.0\n",
    "    # Locate the Community page\n",
    "    myURL = base + \"/community\"\n",
    "    response = requests.get(myURL,auth = (client_id,client_secret))\n",
    "    response.encoding = 'utf-8'\n",
    "    mySoup = BeautifulSoup(response.text, 'html.parser')  #HTML File\n",
    "    found = 0\n",
    "    # Find all hyperlinked Tags\n",
    "    for k in mySoup.find_all('a'):\n",
    "        link = k.get('href')\n",
    "        if link is not None and \"README.md\" in str(link):\n",
    "            my_dict[\"READ\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"CONTRIBUTING.md\" in str(link):\n",
    "            my_dict[\"CONTRIBUTING\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"LICENSE\" in str(link):\n",
    "            my_dict[\"LICENSE\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"CODE_OF_CONDUCT.md\" in str(link):\n",
    "            my_dict[\"CODE\"] = 1\n",
    "            found += 1\n",
    "        if link is not None and \"PULL_REQUEST_TEMPLATE\" in str(link):\n",
    "            my_dict[\"REQUEST\"] = 1\n",
    "            found += 1\n",
    "            \n",
    "    flag_list = ['octicon', 'octicon-check', 'mr-1', 'color-fg-success']\n",
    "    total = 0.0\n",
    "    for k in mySoup.find_all('svg',{\"aria-label\" : \"Added\"}):\n",
    "        if k.get(\"class\") == flag_list:\n",
    "            total = total + 1\n",
    "    \n",
    "    my_dict[\"REMAINING\"] = total - found\n",
    "    return my_dict\n",
    "\n",
    "def checkInput(user_input,keyword=True):\n",
    "    user_input = user_input.strip().lower()\n",
    "    if keyword:\n",
    "        return True if re.match(r'^(?=.*[a-zA-Z])|(?=.*[0-9])', user_input) else False\n",
    "    else:\n",
    "        return True if user_input.startswith( r'https://github.com/') else False\n",
    "\n",
    "def getDataOfSingleRepo(repo):\n",
    "    apiURL = githubAPIadaptor(repo)\n",
    "    r = requests.get(apiURL,auth = (client_id, client_secret))\n",
    "    response_dict = r.json()\n",
    "    update_sec = timestampToSec(response_dict['updated_at'])\n",
    "    push_sec = timestampToSec(response_dict['pushed_at'])\n",
    "    has_wiki = 1 if response_dict['has_wiki'] is True else 0\n",
    "    checkList = checkDocument(repo)\n",
    "    if checkList[\"REMAINING\"] == 2:\n",
    "        checkList[\"ISSUE\"] = 1\n",
    "    elif checkList[\"REMAINING\"] == 1 and response_dict[\"description\"] == None:\n",
    "        checkList[\"ISSUE\"] = 1\n",
    "    elif checkList[\"REMAINING\"] == 1 and response_dict[\"description\"] != None:\n",
    "        checkList[\"Description\"] = 1\n",
    "\n",
    "    row = np.array([update_sec,\n",
    "                    push_sec,\n",
    "                    response_dict['watchers_count'],\n",
    "                    response_dict['stargazers_count'],\n",
    "                    response_dict['forks'],\n",
    "                    response_dict['open_issues'],\n",
    "                    response_dict['subscribers_count'],\n",
    "                    has_wiki,\n",
    "                    checkList.get(\"READ\",0.0),\n",
    "                    checkList.get(\"Description\",0.0),\n",
    "                    checkList.get(\"CONTRIBUTING\",0.0),\n",
    "                    checkList.get(\"LICENSE\",0.0),\n",
    "                    checkList.get(\"CODE\",0.0),\n",
    "                    checkList.get(\"REQUEST\",0.0),\n",
    "                    checkList.get(\"ISSUE\",0.0)\n",
    "                   ])\n",
    "    return row\n",
    "\n",
    "def singleRepoDataProcess(repoData):\n",
    "    res = repoData.copy()\n",
    "    update_gap = time.time() - repoData[0]\n",
    "    push_gap = time.time() - repoData[1]\n",
    "    update_gap = 1 - ((update_gap/oneMonth)*0.1)\n",
    "    push_gap = 1 - ((push_gap/oneMonth)*0.1)\n",
    "    update_gap = '%.4f' % update_gap\n",
    "    push_gap = '%.4f' % push_gap\n",
    "    update_gap = update_gap if float(update_gap) > 0.0 else 0.0\n",
    "    push_gap = push_gap if float(push_gap) > 0.0 else 0.0\n",
    "    res[0] = update_gap\n",
    "    res[1] = push_gap\n",
    "    index = 2\n",
    "    for d in repoData[2:7]:\n",
    "        new_data = 0.1\n",
    "        if d <= 100:\n",
    "            new_data = d/200\n",
    "        elif d >100 and d<=3100:\n",
    "            new_data = 0.5+((d-100)/10000)\n",
    "        elif d >3100 and d<=10000:\n",
    "            new_data = 0.9\n",
    "        else:\n",
    "            new_data = 1\n",
    "        res[index] = new_data\n",
    "        index += 1\n",
    "    return res\n",
    "\n",
    "def searchByKeyword(myKey):\n",
    "    res = getURLgroup(getKeywordResponses(myKey))\n",
    "    arr = np.array([[1.1,2.2,3.3,4.4,5.5,6.6,7.7,8.8,9.9,10.1,11.2,12.3,23.1,3.3,4.4]])\n",
    "    for k,v in res.items():\n",
    "        row = getDataOfSingleRepo(k)\n",
    "        row_n = arr.shape[0] ##last row\n",
    "        arr = np.insert(arr,row_n,[row],axis= 0)\n",
    "    arr = np.delete(arr,0,axis = 0)\n",
    "    normed_data = MaxMinNormalization(arr)\n",
    "    open_issue = 0.4 if arr[6].any() > 0.2 else 0.1\n",
    "    normed_data*=np.array([update,push,watcher,star,fork,\n",
    "                           open_issue,subscriber,has_wiki,\n",
    "                           read,description,contributing,\n",
    "                           license,code_conduct,pull_req,issue])\n",
    "    sum_data = np.sum(normed_data,axis=1)\n",
    "    higestScore = np.where(sum_data==np.max(sum_data))[0][0]\n",
    "    print(\"Best project found by the input:\",list(res.keys())[higestScore])\n",
    "    generateReport(normed_data[higestScore])\n",
    "\n",
    "def searchByURL(myURL):\n",
    "    repoData = getDataOfSingleRepo(myURL)\n",
    "    final_data = singleRepoDataProcess(repoData)*default_weight\n",
    "    generateReport(final_data)\n",
    "\n",
    "def generateReport(final_data):\n",
    "    r_update_time = 10-int(final_data[0]*10)\n",
    "    r_push_time = 10-int(final_data[1]*10)\n",
    "    r_popularity = '%.4f' %sum(final_data[2:7])\n",
    "    total_score = '%.4f' %sum(final_data)\n",
    "    if r_update_time <= 1:\n",
    "        print(\"This project has been updated recently\")\n",
    "    else:\n",
    "        print(\"This project has been updated at least {update_time} month ago\".format(update_time=r_update_time))\n",
    "        \n",
    "    if r_push_time <= 1:\n",
    "        print(\"This project has been pushed recently\")\n",
    "    else:\n",
    "        print(\"This project has been pushed at least {push_time} month ago\".format(push_time=r_push_time))\n",
    "        \n",
    "    print(\"This popularity of this project is {popularity}/3\".format(popularity=r_popularity))\n",
    "    if final_data[7] > 0:\n",
    "        print(\"This project contains wiki page\")\n",
    "    if final_data[8] > 0:\n",
    "        print(\"This project contains README file\")\n",
    "    if final_data[9] > 0:\n",
    "        print(\"This project contains Description\")\n",
    "    if final_data[10] > 0:\n",
    "        print(\"This project contains Contributing\")\n",
    "    if final_data[11] > 0:\n",
    "        print(\"This project contains License\")\n",
    "    if final_data[12] > 0:\n",
    "        print(\"This project contains Code of Conduct\")\n",
    "    if final_data[13] > 0:\n",
    "        print(\"This project contains Pull_Request_Template\")\n",
    "    if final_data[14] > 0:\n",
    "        print(\"This project contains Issue_Template\")\n",
    "    print(\"Total Score for this project: {total}\".format(total = total_score))\n",
    "    if ((r_update_time<=6 or r_push_time<=6) and final_data[8] > 0 and float(r_popularity) >= 0.3 and float(total_score) >= 3.8) or (float(total_score) >= 8):\n",
    "        print(\"This project looks worth downloading to try\")\n",
    "    else:\n",
    "        print(\"This project does not look good, you need to reconsider whether to download it or not\")\n",
    "\n",
    "def start():\n",
    "    use_keyword = True\n",
    "    print(\"Do you want to use a keyword search or a specific repository address？ 1.Keyword 2.Repository address\")\n",
    "    while(True):\n",
    "        ans_q1 = input()\n",
    "        if ans_q1 != \"1\" and ans_q1 != \"2\":\n",
    "            print(\"You must enter a legal parameter: 1 or 2\")\n",
    "        else:\n",
    "            if ans_q1 is \"2\":\n",
    "                use_keyword = False\n",
    "            break\n",
    "\n",
    "    if use_keyword:\n",
    "        the_key = input(\"Please enter your keyword:\")\n",
    "        if checkInput(the_key,use_keyword):\n",
    "            print(\"The report is being generated, please wait a few minutes\")\n",
    "            searchByKeyword(the_key)\n",
    "        else:\n",
    "            print(\"Illegal input\")\n",
    "    else:\n",
    "        the_URL = input(\"Please enter the repository address:\")\n",
    "        if checkInput(the_URL,use_keyword):\n",
    "            print(\"The report is being generated, please wait a few minutes\")\n",
    "            searchByURL(the_URL)\n",
    "        else:\n",
    "            print(\"Illegal input\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to use a keyword search or a specific repository address？ 1.Keyword 2.Repository address\n",
      "1\n",
      "Please enter your keyword:leeds\n",
      "The report is being generated, please wait a few minutes\n",
      "Best project found by the input: https://github.com/Urban-Analytics/dust\n",
      "This project has been updated recently\n",
      "This project has been pushed recently\n",
      "This popularity of this project is 1.7295/3\n",
      "This project contains wiki page\n",
      "This project contains README file\n",
      "This project contains Description\n",
      "This project contains Contributing\n",
      "This project contains License\n",
      "Total Score for this project: 8.3018\n",
      "This project looks worth downloading to try\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "cddedc34-befe-42c0-8899-6b64d1a8edd3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
